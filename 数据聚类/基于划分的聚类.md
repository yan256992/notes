## 3 基于划分的聚类

### 3.1 划分方法概述

**基本思想**：对n个样本化为K个划分，其中的一个划分代表一个簇。

**注意**：

1. 每个簇中至少包含一个样本
2. 每个样本可以属于多个簇

在划分的过程中，首先会随机构建一个初始的划分，之后通过迭代的方式反复将样本中的数据重新分配到新的划分中去，从而改善整体的质量。

<img src="https://gitee.com/yan256992/cloudimages/raw/master/img/image-20210814162135145.png" alt="image-20210814162135145" style="zoom:80%;" />

**不足之处**：

1. 聚类的个数需要自己指定
2. 初始值的设定会对结果造成很大的影响 初始值小了则不能区分某些存在差异的点，初始值大了会导致原本属于同一类划分到不同的类别
3. 只适用于部分类型的数据结构

### 3.2 K均值算法

#### 3.2.1 目标函数

![image-20210814163126593](https://gitee.com/yan256992/cloudimages/raw/master/img/image-20210814163126593.png)

<img src="https://gitee.com/yan256992/cloudimages/raw/master/img/image-20210814163257344.png" alt="image-20210814163257344" style="zoom: 67%;" />

**Lloyd算法**：

1. 分配过程：将每个数据样本分配到与他距离最近的所属类中。
2. 更新过程：将所有的数据分好类之后，对每个类重新计算其中心点，然后再执行第一步

该算法简单易于理解，但是容易陷入局部最优解，而达不到全局最优解。常用的解决方案就是在同一个数据集中多次运行该i算法，选取SSE最小的作为最终的聚类结果。

#### 3.2.2 算法流程

<img src="https://gitee.com/yan256992/cloudimages/raw/master/img/image-20210814163927119.png" alt="image-20210814163927119" style="zoom:67%;" />

#### 3.2.3 性能分析

#### 3.3.4 K的选择

1. 设定一个最大的阈值，让K逐渐增大，观察SSE的变化，选取合适的K。即选取令SSE最小的那个K值

<img src="https://gitee.com/yan256992/cloudimages/raw/master/img/image-20210814170822671.png" alt="image-20210814170822671" style="zoom: 67%;" />

**其他方法：**

<img src="https://gitee.com/yan256992/cloudimages/raw/master/img/image-20210814170941661.png" alt="image-20210814170941661" style="zoom: 50%;" />

<img src="https://gitee.com/yan256992/cloudimages/raw/master/img/image-20210814171003113.png" alt="image-20210814171003113" style="zoom:50%;" />

<img src="https://gitee.com/yan256992/cloudimages/raw/master/img/image-20210814171023552.png" alt="image-20210814171023552" style="zoom:50%;" />

#### 3.2.5 初始中心的选择

##### 3.2.5.1 朴素方法

1. 随机选取K个点为初始中心点，但是这种方法很容易造成局部最优解，导致所得到的聚类最终不是最优的。

针对上面的问题，一个有效的方法就是将K均值算法与聚合层次聚类方法相结合。取一个数据集，首先采用层次聚类技术对他进行一个预聚类分析；之后从聚 类的结果中提取出K个簇，并计算中心点作为初始中心点。最后进行聚类

2. 先从数据集中随机抽取一些子样本集，对每一个子样本集都实施随机选取初始点的K-均值算法。将算法运行后产生的中心点放到一个集合中。构成一个仅由中心点构成的集合。对集合进行聚类分析，将得到的结果作为原数据集初始化的中心点。
3. 选取那些在其领域内有大量数据点并且相互之间分散开的点作为初始点的候选点集。在选取了第一个中心点之后，每个初始中心点的选取都要同时考虑到其周围点的密度以及该点与之前选取的所有点的分散度（即距离d1）

<img src="https://gitee.com/yan256992/cloudimages/raw/master/img/image-20210814172902797.png" alt="image-20210814172902797" style="zoom:50%;" />

##### 3.2.5.2 k-均值++ 算法

> 该算法选择初始中心点的基本思想是 聚类中心点之间的距离要尽可能的远。这样就充分的考虑到数据样本集内所有的样本分布情况

首先会随机的选定一个初始中心点，之后选取距离这个中心点最远的点作为下一个初始点，重复这个过程，直到所有的中心点选择完毕。

在选取的过程中引入了概率的思想，将每个点被选中的概率与其距离最近的已选中心点的距离相联系，距离越大，被选取为聚类中心的概率就越大。

**算法步骤**

<img src="https://gitee.com/yan256992/cloudimages/raw/master/img/image-20210814174011222.png" alt="image-20210814174011222" style="zoom:50%;" />

<img src="https://gitee.com/yan256992/cloudimages/raw/master/img/image-20210814175819494.png" alt="image-20210814175819494" style="zoom: 67%;" />

![image-20210814175839199](https://gitee.com/yan256992/cloudimages/raw/master/img/image-20210814175839199.png)

![image-20210814175854727](https://gitee.com/yan256992/cloudimages/raw/master/img/image-20210814175854727.png)

其中的P(x)就是每个样本被选为下一个聚类中心的概率。

最后一行的Sum是概率px的累加和，用于轮盘法选择出第二个聚类中心。

方法是随机产生出一个0~1之间的随机数，判断它属于哪个区间，那么该区间对应的序号就是被选择出来的第二个聚类中心了。

例如1号点的区间为[0,0.2)，2号点的区间为[0.2, 0.525)。

从上表可以直观的看到第二个初始聚类中心是1号，2号，3号，4号中的一个的概率为0.9。

而这4个点正好是离第一个初始聚类中心6号点较远的四个点。

### 3.3 类K-均值算法

#### 3.3.1 k-中心点算法

K中心点算法与K均值不同的是，选取的中心点是真实的数据，而K均值选取的点有可能是虚拟的点

<img src="C:\Users\25699\AppData\Roaming\Typora\typora-user-images\image-20210814213042490.png" alt="image-20210814213042490" style="zoom:67%;" />

<img src="https://gitee.com/yan256992/cloudimages/raw/master/img/image-20210814213122796.png" alt="image-20210814213122796" style="zoom:67%;" />

<img src="https://gitee.com/yan256992/cloudimages/raw/master/img/image-20210814213140811.png" alt="image-20210814213140811" style="zoom:67%;" />

K-中心点算法具有更高的时间复杂度，为了解决在得到每个簇的最终代表对象的过程需要进行多次交换操作的问题。围绕中心点分割（PAM）的改进k-中心点算法被提出。这个算法将聚类结果的质量用一个评估对象及其代表对象之间的平均相异度函数来估算<img src="https://gitee.com/yan256992/cloudimages/raw/master/img/image-20210814213756239.png" alt="image-20210814213756239" style="zoom: 50%;" />

#### 3.3.2 k-中值算法

> k中值算法采用计算每个簇中值的方法取代了k均值算法中计算簇平均值的方法。相比于簇类所有点的平均值，其中值受到离群点的干扰程度相对较小

<img src="https://gitee.com/yan256992/cloudimages/raw/master/img/image-20210814214718528.png" alt="image-20210814214718528" style="zoom: 67%;" />

相对于均值算法的优点是 对于数据集中包含的噪声点和离群点有着更好的鲁棒性，

![image-20210814215132199](https://gitee.com/yan256992/cloudimages/raw/master/img/image-20210814215132199.png)

3.3.3 k-modes算法

>  可以看成是k-均值算法在非数值集合上的版本。对于非数值类型的数据，该算法采用了差异度的度量方式代替k-均值算法中的距离度量，两个对象的差异度越小，表示他们的距离越小。

<img src="https://gitee.com/yan256992/cloudimages/raw/master/img/image-20210814215639122.png" alt="image-20210814215639122" style="zoom: 67%;" />

#### 3.3.4 模糊k-均值

K-均值算法中，每个对象只能从属于所有类别中的某一类，这种方式成为硬聚类。<u>对于从属于同一类中的所有元素来说，k-均值算法是无法将他们分开的。这种强分配方式在处理一些复杂的数据集合时会造成类别指派不合理，</u>

因此提出了一种结合模糊划分概念的聚类方法被提出，即==模糊k-均值==（FCM）。在该算法中，对于同一个类别，对象都对应一个取值范围在【0,1】的数值，这个值表示该对象从属于某一个类别的可能性，在簇中心点更新的过程中，这个数值反映了该对象对于这一类的贡献程度，根据贡献程度的不同，反映出对象更倾向于分配哪个类别中。

一个对象可以从属于多个类别的聚类方法成为软聚类

> 传统硬聚类算法隶属度只有两个值 0 和 1。 也就是说一个样本只能完全属于某一个类或者完全不属于某一个类。举个例子，把温度分为两类，大于10度为热，小于或者等于10度为冷，这就是典型的“硬隶属度”概念。 那么不论是5度 还是负100度都属于冷这个类，而不属于热这个类的。而模糊集里的隶属度是一个取值在[0 1]区间内的数。一个样本同时属于所有的类，但是通过隶属度的大小来区分其差异。比如5度，可能属于冷这类的隶属度值为0.7,而属于热这个类的值为0.3。

<img src="https://gitee.com/yan256992/cloudimages/raw/master/img/image-20210815113316931.png" alt="image-20210815113316931" style="zoom:50%;" />

<img src="https://gitee.com/yan256992/cloudimages/raw/master/img/image-20210815113450370.png" alt="image-20210815113450370" style="zoom:50%;" />

#### 3.3.5 核k-均值算法

当数据集在密度和形状上有很大差异的时候，标准k-均值算法就无法发挥它的效果了。核k-均值聚类算法主要思想是通过一个非线性映射，将输入空间中的数据点映射到一个高维特征空间中，并选取合适的核函数代替非线性映射的内积，在特征空间综合进行聚类分析。这种将数据映射到高维空间的方法可以突出样本类别之间的特征差异，使得样本在核空间中变得线性可分。

<img src="https://gitee.com/yan256992/cloudimages/raw/master/img/image-20210815120950955.png" alt="image-20210815120950955" style="zoom:50%;" />

#### 3.3.6 二分K-均值

> 首先将所有的点看做一个簇，为了得到最终的k个簇，就将这个簇分裂为两个簇；之后迭代这个过程，直到得到所要求的的k个簇

![image-20210815150524162](https://gitee.com/yan256992/cloudimages/raw/master/img/image-20210815150524162.png)

### 3.3 改进的k-均值算法

#### 3.4.1 概述![image-20210815150816176](https://gitee.com/yan256992/cloudimages/raw/master/img/image-20210815150816176.png)

#### 3.4.2 基于边界值的k-均值算法

![image-20210815152756595](https://gitee.com/yan256992/cloudimages/raw/master/img/image-20210815152756595.png)

